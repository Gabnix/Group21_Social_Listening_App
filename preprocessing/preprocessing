import spacy

# Load small english model: https://spacy.io/models
nlp = spacy.load("en_core_web_sm")

# Read the robotics data from file
with open('robotics_data.txt', 'r', encoding='utf-8') as file:
    robotics_data = file.read()

# Pass the Text to Model
robotics_doc = nlp(robotics_data)

# Get token count before preprocessing
token_count_before = len(robotics_doc)
print('Before PreProcessing n_Tokens: ', token_count_before)

# Removing stopwords and punctuation from the doc
robotics_doc = [token for token in robotics_doc if not token.is_stop and not token.is_punct]

# Get token count after preprocessing
token_count_after = len(robotics_doc)
print('After PreProcessing n_Tokens: ', token_count_after)

# Write the processed tokens and statistics to output file
with open('processed_tokens.txt', 'w', encoding='utf-8') as output_file:
    # Write token counts
    output_file.write(f"Token count before preprocessing: {token_count_before}\n")
    output_file.write(f"Token count after preprocessing: {token_count_after}\n\n")
    output_file.write("Processed tokens:\n")
    output_file.write("-----------------\n")
    
    # Write each processed token on a new line
    for token in robotics_doc:
        output_file.write(f"{token.text}\n")