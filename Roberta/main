# Importing the libraries needed
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
import torch
import seaborn as sns
import transformers
import json
from tqdm import tqdm
from torch.utils.data import Dataset, DataLoader
from transformers import RobertaModel, RobertaTokenizer
import logging
from torch import cuda

# Configure logging
logging.basicConfig(level=logging.ERROR)

# Global constants
MAX_LEN = 256
TRAIN_BATCH_SIZE = 8
VALID_BATCH_SIZE = 4
LEARNING_RATE = 1e-05
TRAIN_SIZE = 0.8

# Sentiment mapping for human-readable output
SENTIMENT_MAP = {
    0: "Very Negative",
    1: "Negative",
    2: "Neutral",
    3: "Positive",
    4: "Very Positive"
}

def setup_device():
    """Set up and return the device for model training."""
    return 'cuda' if cuda.is_available() else 'cpu'

def load_and_preprocess_data():
    """Load and preprocess the training data."""
    train = pd.read_csv('train.tsv', delimiter='\t')
    new_df = train[['Phrase', 'Sentiment']]
    return new_df

def split_data(dataframe):
    """Split data into training and testing sets."""
    train_data = dataframe.sample(frac=TRAIN_SIZE, random_state=200)
    test_data = dataframe.drop(train_data.index).reset_index(drop=True)
    train_data = train_data.reset_index(drop=True)
    
    print("FULL Dataset: {}".format(dataframe.shape))
    print("TRAIN Dataset: {}".format(train_data.shape))
    print("TEST Dataset: {}".format(test_data.shape))
    
    return train_data, test_data

class SentimentData(Dataset):
    def __init__(self, dataframe, tokenizer, max_len):
        self.tokenizer = tokenizer
        self.data = dataframe
        self.text = dataframe.Phrase
        self.targets = self.data.Sentiment
        self.max_len = max_len

    def __len__(self):
        return len(self.text)

    def __getitem__(self, index):
        text = str(self.text[index])
        text = " ".join(text.split())

        inputs = self.tokenizer.encode_plus(
            text,
            None,
            add_special_tokens=True,
            max_length=self.max_len,
            pad_to_max_length=True,
            return_token_type_ids=True
        )
        ids = inputs['input_ids']
        mask = inputs['attention_mask']
        token_type_ids = inputs["token_type_ids"]

        return {
            'ids': torch.tensor(ids, dtype=torch.long),
            'mask': torch.tensor(mask, dtype=torch.long),
            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),
            'targets': torch.tensor(self.targets[index], dtype=torch.float)
        }

def create_dataloaders(train_data, test_data, tokenizer):
    """Create training and testing dataloaders."""
    training_set = SentimentData(train_data, tokenizer, MAX_LEN)
    testing_set = SentimentData(test_data, tokenizer, MAX_LEN)

    train_params = {'batch_size': TRAIN_BATCH_SIZE,
                   'shuffle': True,
                   'num_workers': 0}

    test_params = {'batch_size': VALID_BATCH_SIZE,
                  'shuffle': True,
                  'num_workers': 0}

    training_loader = DataLoader(training_set, **train_params)
    testing_loader = DataLoader(testing_set, **test_params)
    
    return training_loader, testing_loader

class RobertaClass(torch.nn.Module):
    def __init__(self):
        super(RobertaClass, self).__init__()
        self.l1 = RobertaModel.from_pretrained("roberta-base")
        self.pre_classifier = torch.nn.Linear(768, 768)
        self.dropout = torch.nn.Dropout(0.3)
        self.classifier = torch.nn.Linear(768, 5)

    def forward(self, input_ids, attention_mask, token_type_ids):
        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)
        hidden_state = output_1[0]
        pooler = hidden_state[:, 0]
        pooler = self.pre_classifier(pooler)
        pooler = torch.nn.ReLU()(pooler)
        pooler = self.dropout(pooler)
        output = self.classifier(pooler)
        return output

def setup_model(device):
    """Initialize and setup the model."""
    model = RobertaClass()
    model.to(device)
    return model

def calculate_accuracy(preds, targets):
    """Calculate accuracy between predictions and targets."""
    n_correct = (preds==targets).sum().item()
    return n_correct

def train_epoch(model, training_loader, optimizer, loss_function, device, epoch):
    """Train the model for one epoch."""
    tr_loss = 0
    n_correct = 0
    nb_tr_steps = 0
    nb_tr_examples = 0
    model.train()
    
    for _, data in tqdm(enumerate(training_loader, 0)):
        ids = data['ids'].to(device, dtype=torch.long)
        mask = data['mask'].to(device, dtype=torch.long)
        token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)
        targets = data['targets'].to(device, dtype=torch.long)

        outputs = model(ids, mask, token_type_ids)
        loss = loss_function(outputs, targets)
        tr_loss += loss.item()
        big_val, big_idx = torch.max(outputs.data, dim=1)
        n_correct += calculate_accuracy(big_idx, targets)

        nb_tr_steps += 1
        nb_tr_examples += targets.size(0)
        
        if _%5000==0:
            loss_step = tr_loss/nb_tr_steps
            accu_step = (n_correct*100)/nb_tr_examples 
            print(f"Training Loss per 5000 steps: {loss_step}")
            print(f"Training Accuracy per 5000 steps: {accu_step}")

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')
    epoch_loss = tr_loss/nb_tr_steps
    epoch_accu = (n_correct*100)/nb_tr_examples
    print(f"Training Loss Epoch: {epoch_loss}")
    print(f"Training Accuracy Epoch: {epoch_accu}")

def predict_sentiment(model, tokenizer, device, text):
    """
    Predict sentiment for a single input phrase.
    
    Args:
        model: Trained RoBERTa model
        tokenizer: RoBERTa tokenizer
        device: Computing device (CPU/GPU)
        text: Input text to analyze
        
    Returns:
        dict: Dictionary containing prediction details
    """
    model.eval()  # Set model to evaluation mode
    
    # Tokenize the input text
    inputs = tokenizer.encode_plus(
        text,
        None,
        add_special_tokens=True,
        max_length=MAX_LEN,
        pad_to_max_length=True,
        return_token_type_ids=True
    )
    
    # Convert inputs to tensors and move to device
    ids = torch.tensor([inputs['input_ids']], dtype=torch.long).to(device)
    mask = torch.tensor([inputs['attention_mask']], dtype=torch.long).to(device)
    token_type_ids = torch.tensor([inputs["token_type_ids"]], dtype=torch.long).to(device)

    # Get prediction
    with torch.no_grad():
        outputs = model(ids, mask, token_type_ids)
        prediction = torch.nn.functional.softmax(outputs, dim=1)
        prediction_prob, prediction_idx = torch.max(prediction, dim=1)
    
    return {
        'text': text,
        'sentiment': SENTIMENT_MAP[prediction_idx.item()],
        'confidence': prediction_prob.item(),
        'probabilities': {
            SENTIMENT_MAP[i]: prob.item()
            for i, prob in enumerate(prediction[0])
        }
    }

def main():
    """Main execution function."""
    # Setup
    device = setup_device()
    tokenizer = RobertaTokenizer.from_pretrained('roberta-base', truncation=True, do_lower_case=True)
    
    # Data preparation
    data = load_and_preprocess_data()
    train_data, test_data = split_data(data)
    training_loader, testing_loader = create_dataloaders(train_data, test_data, tokenizer)
    
    # Model setup
    model = setup_model(device)
    loss_function = torch.nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)
    
    # Training
    EPOCHS = 1
    for epoch in range(EPOCHS):
        train_epoch(model, training_loader, optimizer, loss_function, device, epoch)
    
    # Example of single prediction
    sample_text = "The movie was absolutely fantastic, I enjoyed every minute of it!"
    result = predict_sentiment(model, tokenizer, device, sample_text)
    
    print("\nSingle Sample Prediction Example:")
    print(f"Text: {result['text']}")
    print(f"Predicted Sentiment: {result['sentiment']}")
    print(f"Confidence: {result['confidence']:.2%}")
    print("\nDetailed Probabilities:")
    for sentiment, prob in result['probabilities'].items():
        print(f"{sentiment}: {prob:.2%}")

if __name__ == "__main__":
    main()
